export default {
  // 任务详情
  taskInfo: {
    instanceInfo: 'Instances',
    runStatus: 'Status',
    place_runStatus: 'Select status',
    timeRange: 'Scheduled time',
    batchClear: 'Batch rerun',
    batchStop: 'Batch stop',
    instanceId: 'ID',
    runNum: 'Runs',
    bout: '',
    inputTime: 'Scheduled time',
    taskStartTime: 'Start time',
    taskEndTime: 'End Time',
    taskTime: 'Duration',
    restartCheck: 'Restart check',
    repeat: 'Rerun',
    more: 'More',
    kibana: 'Kibana Logs',
    viewScript: 'Script Preview',
    viewSql: 'SQL Preview',
    dataLineage: 'Lineage',
    instanceTime: 'Instance time',
    place_instanceTime: 'The instance time of the current task',
    upstream: 'Upstream',
    downstream: 'Downstream',
    depth: 'Depth',
    internalData: 'Internal data',
    externalData: 'External data',
    taskName: 'Task name',
    dataSetType: 'Data type',
    external: 'External',
    internal: 'Internal',
    dataSetName: 'Data name',
    path: 'Data path',
    readyTime: 'Ready time',
    omitInstance: 'This node represents the omitted intermediate instance in the chain',
    taskId: 'Task ID',
    currentInstance: 'Current instance',
    dataDate: 'Data time',
    instanceDate: 'Instance time',
    dataSetOwner: 'Data owner',
    taskOwner: 'Task owner',
    isRepeat: 'Confirm to rerun?',
    setSuccess: 'Mark as succeeded',
    setFail: 'Mark as failed',
    executionSuccess: 'Succeeded run',
    executionFailed: 'Failed run',
    linkAnalysis: 'Chain Analysis',
    instanceView: 'Matrix View',
    recent: 'Num',
    executionTime: 'Duration',
    name: 'Name',
    startTime: 'Start time',
    endTime: 'End time',
    owner: 'Owner',
    log: 'Logs',
    ganttChart: 'Gantt Chart',
    status: 'Status',
    success: 'Succeeded',
    fail: 'Failed',
    new: 'Newly',
    repeatTips: 'Whether the rerun task is checking upstream during execution',
    relyTips: 'Used for sequential execution of self dependent task',
    dataTips: 'Solid and dashed borders are used to distinguish internal or external data; The gray background indicates that the task is offline; The border color is used to distinguish the current instance status',
    online: 'Offline',
    omit: 'Omitted nodes in the chain',
    noUpInstance: 'No upstream instances found',
    runRecord: 'Operation records',
    schedulingType: 'Scheduling type',
    complement: 'Complement',
    scheduling: 'Scheduling',
    lookLog: 'View log',
    isBatchClear: 'Confirm to batch rerun?',
    selectInstance: 'Select at least one instance',
    yesTime: 'Completion time'
  },
  taskStatusMenu: {
    checkUp: 'Checking upstream',
    queuing: 'Queuing',
    running: 'Running',
    success: 'Succeeded',
    error: 'Failed',
    notReady: 'Not ready',
    ready: 'Ready',
    stopCheckUp: 'Stopping checking upstream',
    waitRepeat: 'Waiting for rerun'
  },
  task: {
    lang: 'en',
    // 翻译补充1
    taskAdd1: 'Internal dependency',
    taskAdd2: 'Upstream tasks',
    taskAdd3: 'Input task ID/name/output',
    taskAdd4: 'Select a task',
    taskAdd5: 'Offset instructions:',
    taskAdd6: '1) Meaning:',
    taskAdd7: 'How many scheduling periods between the scheduled time of the current task and the scheduled time of the upstream task;',
    taskAdd8: '2) Effect:',
    taskAdd9: 'Scheduled time of the current task + offset * period of the dependent task = scheduled time of the dependent task.',
    taskAdd10: '3) Example:',
    taskAdd11: 'Task B depends on task A. Today is November 18th, and the scheduled time of B is November 18th.',
    taskAdd12: '-- If the offset is 0, the scheduled time of task A = November 18th + 0 = November 18th',
    taskAdd13: '-- If the offset is -1, the scheduled time of task A = November 18th - 1 = November 17th',
    taskAdd14: 'More information here',
    taskAdd15: 'Frequency',
    taskAdd16: 'Week',
    taskAdd17: 'Date',
    taskAdd18: 'After refresh, pre-dependencies will be re-identified based on the setting of the previous step.',
    taskAdd19: 'Refresh tip',
    taskAdd20: 'Configure the week?',
    taskAdd21: 'Offset mode',
    taskAdd22: 'Custom mode',
    taskAdd23: 'Offset mode cannot be empty',
    taskAdd24: 'The custom mode cannot be empty',
    taskAdd25: 'Name/ID',
    taskAdd26: 'save check',
    taskAdd27: 'pass',
    taskAdd28: 'remind',
    taskAdd29: 'Checking',
    taskAdd30: 'Reminder message',
    taskAdd31: 'For reminder information, please contact the administrator of the current tenant (xuebotao) to apply.',
    taskAdd32: 'Go apply',
    taskAdd33: 'to modify',
    taskAdd34: 'Please ensure that the above warning items have been dealt with before "going online"',
    taskAdd35: 'Fail',
    taskAdd36: 'card',
    taskAdd37: 'Excuting an order',
    taskAdd38: 'Please enter code location',
    taskAdd39: 'Please enter the training parameter file',
    taskAdd40: 'Please enter the execution command',
    taskAdd41: 'Please enter CPU resources',
    taskAdd42: 'Please enter memory resources',
    taskAdd43: 'Please enter GPU resources',
    taskAdd44: 'Please complete the information!',
    // 任务列表及任务卡片页
    tagSelectPh: 'Select a label',
    allTag: 'Task label',
    collectText: 'Add to my favorites',
    myCollectText: 'My favorites',
    myTagText: 'My labels',
    editTip: 'Right click label to edit',
    addText: 'New',
    publicTagText: 'Labels shared with me',
    searchLabeld: 'ID/Name',
    taskTypelabel: 'Type',
    taskStatusLabel: 'State',
    taskGuidsLabel: 'Input dataset',
    taskOn: 'Put the task online',
    taskDown: 'Take the task offline',
    taskTableName: 'Output dataset',
    taskTableNamePh: 'Input ID/name',
    taskOwner: 'Owner',
    taskOwnerPh: 'Input owner',
    onlySelf: 'My own tasks',
    addTask: 'New',
    allTaskNums: 'Total',
    statusPre: 'The status of recent task instances is:',
    sucessStatus: 'Succeeded',
    failedStatus: 'Failed',
    runningStatus: 'Running',
    columsText: 'Customize',
    taskList1: 'ID',
    taskList2: 'Name',
    taskType: 'Type',
    taskList3: 'Recent instance status',
    taskList4: 'Scheduled time: ',
    taskList5: 'Status: ',
    taskList6: 'Start time: ',
    taskList7: 'End time: ',
    taskList8: 'Duration: ',
    taskList9: 'Output dataset',
    taskList10: 'Input dataset',
    updateTime: 'Update time',
    taskStatus: 'State',
    taskList11: 'Online or offline',
    taskList12: 'Description',
    taskList13: 'Options',
    commonSmall: 'Exit full screen',
    commonBig: 'Full screen',
    taskList14: 'Go to the workflow page',
    taskEdit: 'Edit ',
    taskShow: 'Check',
    taskCopy: 'Copy',
    taskList15: 'Complement',
    taskList151: 'Manual Execution',
    taskVersion: 'Versions',
    taskDelet: 'Delete',
    taskList16: 'Whether to notify downstream tasks',
    taskTrue: 'Yes',
    taskFalse: 'No',
    taskCancel: 'Cancel',
    taskOk: 'OK',
    taskCancel1: 'Cancel',
    taskOk1: 'OK',
    taskList17: 'Task instance status flow',
    taskList18: 'Collapse',
    taskList19: 'Expand',
    taskList20: 'Confirm to delete this label?',
    taskList21: 'Tips',
    taskList22: 'Successfully deleted!',
    taskList23: 'Add to my favorites',
    taskList24: 'Successfully added!',
    taskList25: 'Remove from my favorites',
    taskList26: 'Successfully canceled!',
    copy: 'Copy',
    taskList27: 'Open in new tab',
    taskList28: 'Confirm to release?',
    taskList29: 'Successfully operated!',
    taskList30: 'Confirm to put it online?',
    taskList31: 'Confirm to execute this task?',
    duringTime: 'Time range',
    to: 'to',
    startTime: 'Start time',
    endTime: 'End Time',
    taskList32: 'Method',
    taskList33: 'Common',
    taskList34: 'Deep',
    taskList35: 'Downstream',
    taskList36: 'Select all',
    taskList37: 'Please select downstream tasks',
    taskList38: 'Only downstream tasks of the current user group can be selected',
    taskList39: 'Notify downstream task owner',
    taskList40: 'Please select start and end time',
    taskList41: 'Please select the method',
    taskList42: 'Please select downstream tasks',
    taskList43: 'Please input the label name, no more than 128 characters',
    taskList44: 'Please select the tasks belonging to this label (up to 500 can be selected)',
    taskList45: 'Make public',
    taskList46: 'People can edit the public label, but delete is not allowed',
    labelName: 'Name',
    linkTask: 'Associated tasks',
    publickTo: 'Share to',
    taskList47: 'Input sharer names',
    taskrule1: '[Only a-z, A-Z, 0-9 or _ are allowed], length 2-128',
    taskrule2: 'Please input people to share with',
    editTitle: 'Edit label',
    addTitle: 'New label',
    taskList48: 'The number of associated tasks cannot exceed 500',
    taskList49: 'Versions',
    taskList50: 'Record the saved task history versions and corresponding operations.',
    taskList51: 'Operation type:',
    taskList52: 'Version number',
    taskList53: 'Jump to the page of the current version',
    taskList54: ' (current)',
    taskList55: 'Time',
    taskList56: 'Type',
    taskList57: 'Operator',
    taskList58: 'Description',
    taskList59: 'Switch',
    taskList60: 'Confirm to switch to current version?',
    taskStep1: 'Set task configuration',
    taskStep2: 'Set task dependency',
    taskStep3: 'Select template',
    taskStep4: 'Select data source',
    taskStep5: 'Select the desired template',
    taskStep6: 'Configure the data source',
    taskStep7: 'Configure task',
    taskStep8: 'Create table',
    taskStep9: 'Configure input, field mapping, executing logic, etc.',
    taskStep10: 'Input table info',
    taskStep11: 'Configure scheduling',
    taskStep12: 'Set table info',
    taskStep13: 'Configure cron, dependency, output, etc.',
    taskStep14: 'Add table maintaining info',
    taskStep15: 'Preview',
    taskStep16: 'Preview results and verification',
    taskCardAdd: 'This template is suitable for synchronizing file data in various formats (such as csv, json, paruet, orc, etc.) stored in object storage (such as AWS OSS, S3, etc.) or distributed file storage (such as HDFS, etc.) to Lakehouse In the lake warehouse.',
    taskCard1: 'Data Ingestion',
    taskCard2: 'Ingest DB data to the LakeHouse.',
    taskCard3: 'Ingest MQ data to the LakeHouse.',
    taskCard4: 'Data Transformation',
    taskCard5: 'Intra-cloud data transformation based on SparkSQL.',
    taskCard6: 'Inter-cloud data transfer.',
    taskCard7: 'Data Export',
    taskCard8: 'Export data from LakeHouse to MySQL.',
    taskCard9: 'Export data from LakeHouse to ClickHouse.',
    taskCard10: 'Export data from LakeHouse to Doris.',
    taskCard11: 'Export data from LakeHouse to Redis.',
    taskCard12: 'Export data from LakeHouse to Redshift.',
    taskCard13: 'Export data from LakeHouse to ShareStore/TiKV.',
    taskCard14: 'Export data from LakeHouse to File via Email.',
    taskCard15: 'Data Ingestion',
    taskCard16: 'Execute Spark Jar for complex data transformation.',
    taskCard17: 'Shell/Python Script',
    taskCard18: 'Execute python and shell script.',
    taskCard19: 'Lakehouse ETL',
    taskCard20: 'Execute Flink Jar for streaming processing.',
    taskCard21: 'Execute Flink SQL for streaming processing.',
    stepTip: 'General Template',
    stepMore: 'more',
    taskFile1: 'Folder',
    taskFile2: 'Label',
    taskFile3: 'All tags',
    taskFile4: 'Please select a label',
    taskFile5: 'Right click to edit label',
    taskFile6: 'Associated tasks',
    taskFile7: 'Create new task',
    taskFile8: 'successfully deleted',
    taskFile9: 'Successfully modified',
    taskFile10: 'Created successfully',
    // 高级设置非流式任务
    setting1: 'Collaborator',
    setting2: 'Input collaborator',
    setting3: 'You can create user group in the User Management module',
    setting4: 'User group',
    setting5: 'Compute engine',
    setting6: 'Select which computing engine is used when executing the task. Different templates support different engines. After selecting the engine, the task will be submitted to the corresponding cluster. You can choose based on your needs.',
    setting7: 'Select engine',
    setting8: 'Cluster type',
    setting9: 'Select cluster type',
    setting10: 'Resource size',
    setting11: 'Different resource size leads to different cost.',
    setting12: 'Size',
    setting13: 'Mode',
    setting14: 'Cost($/h)',
    setting15: 'Retry after timeout',
    setting16: 'When the task running time exceeds this value, it will be automatically retried. An alarm will be triggered, but the task will not be killed. Default is 0, which means the system set the value automatically based on the task history.',
    setting17: 's',
    setting18: 'Retry limit',
    setting19: 'Value range: 0~100',
    setting20: 'Parallelism',
    setting21: 'Specifies the number of instances to run in parallel',
    setting23: 'Note that when the parallelism exceeds 5, the cluster resource is likely exhausted',
    setting24: 'Start time',
    setting25: 'End time',
    setting26: 'Economic mode',
    setting27: 'Cost first mode: Spot node is used; Performance first mode: OnDemand node is used',
    setting28: 'Cost first',
    setting29: 'Performance first',
    setting30: '',
    setting31: 'Control which node pod is scheduled on',
    setting32: 'Label',
    setting33: 'For pod discovery',
    setting34: '',
    setting35: 'Set pod environment variables',
    setting36: '',
    setting37: 'Configure the resources requested by the pod, including CPU, memory, etc.',
    setting38: '',
    setting39: 'For data storeage',
    setting40: '',
    setting41: 'Used to mount data volumes',
    setting42: '',
    setting43: 'Used to inject annotation information',
    setting44: 'The following examples are Spark parameters',
    setting45: 'Other parameters',
    setting46: 'Input parameter',
    setting47: 'Input the key of the parameter first!',
    // 高级设置流式任务
    settingStreaming1: 'Mode',
    settingStreaming2: 'Interval',
    settingStreaming3: 'Timeout',
    settingStreaming4: 'Mode',
    settingStreaming5: 'Autoscaling mode',
    settingStreaming6: 'Create a new one?',
    settingStreaming7: 'CDC startup mode',
    settingStreaming8: 'Startup mode',
    settingStreaming9: 'Configuration option is ',
    settingStreaming10: ', which specifies the startup mode of the MySQL CDC. Valid mode include:',
    settingStreaming11: '(Default): Perform an initial snapshot of the monitored tables on startup and then continue reading the latest binlog.',
    settingStreaming12: ': Skip the snapshot phase and start reading from the earliest binlog position that can be read.',
    settingStreaming13: ': When first started, snapshots are never performed on the monitored tables and only starts reading from the end of the binlog, which means that only data changes after the connector is started can be read.',
    settingStreaming14: 'Select startup mode',
    settingStreaming15: 'Advanced',
    settingStreaming16: 'Job type',
    settingStreaming17: 'Batch',
    settingStreaming18: 'Stream',
    settingStreaming19: 'Mail',
    settingStreaming20: 'Input the email to receive alarms',
    settingStreaming21: 'The following parameters only take effect for individual job',
    settingStreaming22: 'For identity authentication. The account is provided by the admin and should include permissions on the directory [obs://cbs-flink-sg/main-sg2-prod/]',
    settingStreaming23: 'Input IAM',
    settingStreaming24: 'For identity authentication. The account is provided by the admin and should include permissions on the directory [s3a://cbs.flink.ap-southeast-1/ads-sg1-prod/]',
    settingStreaming25: 'core',
    settingStreaming26: 'Alarms',
    settingStreamingTime: 'Timeout',
    settingStreaming27: 'Specify the method used to notify when the task instance is failed.',
    settingStreaming28: 'Manner',
    settingStreaming29: 'IM',
    settingStreaming30: 'Phone',
    settingStreaming31: 'Mail',
    settingStreamingAdd1: 'Enterprise WeChat',
    settingStreaming32: 'Notify collaborators',
    settingStreaming33: 'Jar file',
    settingStreaming34: 'Version',
    settingStreaming35: 'Economic mode',
    settingStreaming36: 'Cost first',
    settingStreaming37: 'Choose a strategy',
    settingStreaming38: 'Suggested strategies for operator operations',
    settingStreaming39: 'kafka consumption delay strategy',
    settingStreaming40: 'cyclical strategies',
    settingStreaming41: 'topic name',
    settingStreaming42: 'Input the topic name',
    settingStreaming43: 'KAFKA cluster',
    settingStreaming44: 'Input the KAFKA cluster',
    settingStreaming45: 'Cooling time',
    settingStreaming46: 'scaling tolerance',
    settingStreaming47: 'Minimum degree of parallelism',
    settingStreaming48: 'Maximum parallelism',
    settingStreaming52: 'consumer group',
    settingStreaming53: 'Maximum number of delayed items',
    settingStreaming54: 'Minimum number of delays',
    settingStreaming55: 'delay',
    settingStreaming57: 'Scaling ratio',
    settingStreaming60: 'time interval',
    settingStreaming61: 'Adding is only allowed if the end time is less than 23',
    settingStreaming62: 'Parallelism',
    settingStreaming63: 'Select scaling mode',
    settingStreaming64: 'Select cluster',
    settingStreaming65: 'Input your email',
    settingStreaming66: 'Select checkpoint mode',
    settingStreaming67: 'Input the interval',
    settingStreaming68: 'Input the timeout',
    settingStreaming69: 'Input the cooling time',
    settingStreaming70: 'Input scaling tolerance',
    settingStreaming71: 'Input the minimum parallelism',
    settingStreaming72: 'Input the maximum parallelism',
    settingStreaming73: 'Input consumer group',
    settingStreaming74: 'Input delay time',
    settingStreaming75: 'Input the scaling ratio',
    settingStreaming76: 'Input the minimum number of delays',
    settingStreaming77: 'Input the maximum number of delays',
    settingStreaming78: 'Please select the version first!',
    settingStreaming79: 'Periodic strategy. The end time of the interval must be 24',
    settingStreaming80: 'There are required fields that are not filled in',
    // step3任务配置页
    stepThree1: 'Basics',
    stepThree2: 'Name',
    stepThree3: 'Only offline task name can be updated',
    stepThree4: 'Description',
    stepThree5: 'Scheduling',
    stepThree6: 'Manner',
    stepThree7: 'Periodic',
    stepThree8: 'Manual',
    stepThree9: '',
    stepThree10: 'Mode',
    stepThree11: 'Select a scheduling mode',
    stepThree12: '',
    stepThree13: '',
    stepThree14: 'Select an interval',
    stepThree15: 'Every',
    stepThree16: 'minutes',
    stepThree17: 'minute',
    stepThree18: 'At',
    stepThree19: '',
    stepThree20: 'On',
    stepThree21: '',
    stepThree24: 'Advanced',
    stepThree25: 'Start',
    stepThree26: 'End',
    stepThree27: 'Select start time',
    stepThree28: 'Select end time',
    stepThree29: 'Which hour',
    stepThree30: 'Time interval',
    stepThree31: 'Select a time interval',
    stepThree32: 'Hour',
    stepThree33: 'Cron expression',
    stepThree34: 'View UTC0 time',
    stepThree35: 'View the next 7 scheduling time',
    stepThree36: 'Pre-dependency',
    stepThree37: 'If pre-dependencies are set, the task will only be executed when the pre-dependencies are statisfied',
    stepThree38: 'Type',
    stepThree39: 'Internal',
    stepThree40: 'Pre-dependencies are the tasks or data sets in this platform',
    stepThree41: 'External',
    stepThree42: 'Pre-dependencies are data sets outside this platform',
    stepThree43: 'Internal dependency',
    stepThree44: '',
    stepThree45: 'ID',
    stepThree46: '(Deleted)',
    stepThree47: 'Name',
    stepThree48: 'Schedule',
    stepThree49: 'Output data set',
    stepThree50: 'Offset',
    stepThree51: 'Tips',
    stepThree52: 'Setting',
    stepThree53: 'Offset',
    stepThree54: 'Specify',
    stepThree55: 'External dependency',
    stepThree56: 'Data',
    stepThree57: 'Select',
    stepThree58: 'Input',
    stepThree59: 'Select a data set',
    stepThree60: 'Input a data set',
    stepThree61: 'Select a data set',
    stepThree62: 'Database',
    stepThree63: 'Select a database',
    stepThree64: 'Table',
    stepThree65: 'Select a table',
    stepThree66: 'loading...',
    stepThree67: 'Data type',
    stepThree68: 'External',
    stepThree69: 'Internal',
    stepThree70: 'Schedule',
    stepThree71: 'Select a scheduling',
    stepThree72: 'Offset',
    stepThree73: 'Specify data offsets that the task relies on. Negative means preceding, positive means succeeding',
    stepThree74: 'Single',
    stepThree75: 'Continuous',
    stepThree76: 'Offset:',
    stepThree77: 'Description',
    stepThree78: 'Ready time',
    stepThree79: 'When data is external, the ready time indicates when the data is ready to use',
    stepThree80: 'Ready condition',
    stepThree81: 'Output data set',
    stepThree82: 'Output data set description',
    stepThree83: 'Tip',
    stepThree84: 'Database',
    stepThree85: 'Table',
    stepThree86: 'Generate success mark',
    stepThree87: 'Cost share',
    stepThree88: 'Department',
    stepThree89: 'Department',
    stepThree90: 'Select department',
    stepThree91: 'Proportion',
    stepThree92: 'Input proportion',
    stepThree93: '',
    stepThree94: 'When a downstream task needs to depend on the current task, the downstream task can add output data set as "pre-dependency condition".',
    stepThree95: 'Specify a path to save a success mark of the current task. When a downstream task needs to depend on the current task, it only needs to depend on the mark.',
    stepThree96: 'Scheduled alarm',
    stepThree97: 'During each scheduling period of the task, when the "trigger condition" is met at the specified "time", an alarm will be triggered.',
    stepThree98: 'Succeeded',
    stepThree99: 'Failed',
    stepThree100: 'Retried',
    stepThree101: 'Started',
    stepThree102: 'Manner',
    stepThree103: 'Time',
    stepThree104: 'Select',
    stepThree105: 'No.',
    stepThree106: 'Period',
    stepThree107: 'After this scheduling period',
    stepThree108: 'Trigger condition',
    stepThree109: 'Select an alarm manner',
    stepThree110: 'The name can only contain a-z, A-Z, 0-9 or - and should start with characters, up to 45 characters',
    stepThree111: 'The name can only contain a-z, A-Z, 0-9, - or _ and should start with characters, up to 60 characters',
    stepThree112: 'AWS US East',
    stepThree113: 'AWS Singapore',
    stepThree114: 'Huawei Singapore',
    stepThree145: 'This scheduling period',
    stepThree146: 'After this scheduling period',
    stepThree147: 'Task is not started',
    stepThree148: 'Task is not succeeded',
    stepThree149: 'Input a task name',
    stepThree150: 'Please select the data output granularity',
    stepThree151: 'Select a scheduling period',
    stepThree152: 'Select a specific time',
    stepThree153: 'Select start time',
    stepThree154: 'Please select a time interval',
    stepThree155: 'Select end time',
    stepThree156: 'Select a scheduled time',
    stepThree157: 'Last day of every month',
    stepThree158: 'Every month',
    stepThree159: 'Date',
    stepThree160: 'You can only input up to 45 characters',
    stepThree161: 'You can only input up to 60 characters',
    stepThree162: 'Select the table first',
    stepThree163: 'Select the query ID first',
    stepThree164: 'Task ID',
    stepThree165: 'has been added',
    stepThree166: 'UTC0 time',
    stepThree167: 'Cron expression:',
    stepThree168: 'The next 7 scheduling time',
    stepThree169: 'Cost sharing supports up to 4 departments',
    stepThree170: 'At least one cost sharing is needed',
    // setp4预览
    // setp4预览
    stepView1: 'Source table columns',
    stepView2: 'Target table columns',
    stepView3: 'Conlumn information',
    stepView4: 'Name',
    stepView5: 'Code',
    stepView6: 'Data dependency',
    stepView7: 'Output dataset',
    stepView8: 'Source data',
    stepView9: 'Alarms',
    stepView10: 'Trigger',
    stepView11: 'Table',
    stepView12: 'File',
    stepView13: 'Visualization',
    stepView14: 'SQL',
    stepView15: 'LakeHouse table',
    stepView16: 'Storage files',
    stepView17: 'Artifact',
    stepView18: 'Version',
    stepView19: 'Sync type',
    stepView20: 'Source path',
    stepView21: 'Filter files in this directory',
    stepView22: 'Region',
    stepView23: 'Target path',
    stepView24: 'Delete target file',
    stepView25: 'name',
    stepView26: 'Source table',
    stepView27: 'Partition',
    stepView28: 'Database',
    stepView29: 'Table',
    stepView30: 'Specific time',
    stepView31: 'ClickHouse datasource',
    stepView32: 'ClickHouse database',
    stepView33: 'ClickHouse table',
    stepView34: 'MySQL datasource',
    stepView35: 'MySQL database',
    stepView36: 'MySQL table',
    stepView37: 'Region',
    stepView38: 'Doris datasource',
    stepView39: 'Doris database',
    stepView40: 'Doris table',
    stepView41: 'Redis server',
    stepView42: 'Port',
    stepView43: 'Expiration',
    stepView44: 'Redshift server',
    stepView45: 'Database',
    stepView46: 'Whether to generate a success mark',
    stepView47: 'Parth',
    stepView48: 'File',
    stepView49: 'Output data type',
    stepView50: 'Source data type',
    stepView51: 'Storage path',
    stepView52: 'Sharestore address',
    stepView53: 'Sharestore cluster',
    stepView54: 'Sharestore zookeeper',
    stepView55: 'Target table',
    stepView56: 'TiKV address',
    stepView57: 'Target table',
    stepView58: 'TiKV type',
    stepView59: 'Source type',
    stepView60: 'Kafka address',
    stepView61: 'Metadata display mode',
    stepView62: 'Partition key',
    stepView63: 'LakeHouse region',
    stepView64: 'LakeHouse database',
    stepView65: 'LakeHouse table',
    stepView66: 'Column information',
    stepView67: 'MySQL data source',
    stepView68: 'MySQL database',
    stepView69: 'MySQL table',
    stepView70: 'LakeHouse database',
    stepView71: 'LakeHouse table',
    stepView72: 'Partition',
    stepView73: 'Table sync',
    stepView74: 'Sync type',
    stepView75: 'LakeHouse table prefix (optional)',
    stepView76: 'Source data source',
    stepView77: 'Database',
    stepView78: 'Script type',
    stepView79: 'Mirror address',
    stepView80: 'Dependent artifacts',
    stepView81: 'File address',
    stepView82: 'File type',
    stepView83: 'Target type',
    stepView84: 'Theme',
    stepView85: 'Text',
    stepView86: 'Mail',
    stepView87: 'Attachment format',
    stepView88: 'Code location',
    stepView89: 'Parameter file',
    stepView90: 'CPU',
    stepView91: 'Memory',
    stepView92: 'GPU',
    stepView93: 'Jar file',
    stepView94: 'Jar version',
    stepView95: 'Jar address',
    stepView96: 'Input dataset',
    stepView97: 'Output dataset',
    // 任务组件I
    // iptTable
    ipt1: '(update)',
    ipt2: '(delete)',
    ipt3: 'Confirm to delete this column?',
    ipt4: 'Decimal type customization',
    // ConFun
    conFun1: 'Input column',
    conFun2: ' function mapping',
    conFun3: 'Selected functions',
    conFun4: 'Function name:',
    conFun5: 'Parameter:',
    conFun6: 'Name',
    conFun7: 'Select a function',
    conFun8: 'Help',
    conFun9: 'Please select',
    conFun10: 'Please input',
    conFun11: 'Close',
    conFun12: 'Extract a substring of specified length from the specified position (inclusive) of the column. Throw an exception if the starting position is illegal. If the column is null, return the column without change.',
    conFun13: 'The starting position',
    conFun14: 'Target string length',
    conFun15: 'If the length of the source string is less than the target length, pad characters are added according to the specified position. If it is longer, truncate it directly from the right hand side. If the column is null, it is converted to an empty string for padding.',
    conFun16: 'You can input "l" or "r". "l" means pad at the beginning, "r" means pad at the end',
    conFun18: 'Pad characters',
    conFun19: 'Replace a substring of specified length from the specified position (inclusive) of the column. Throw an exception if the starting position is illegal. If the column is null, return the column without change.',
    conFun20: 'The string that needs to be replaced',
    conFun21: 'The string to replace',
    conFun22: 'If the regular expression is matched, NULL is returned, indicating that the row is filtered. When the expression does not match, the row is retained.',
    conFun23: 'The following operators are supported: like, not like, >, =, <, >=, !=, <=',
    conFun24: 'Regular expression (Java) or specific value',
    // genPatchForm
    gpForm1: 'Generate success mark',
    gpForm2: 'Mark file path:',
    gpForm3: 'Input file path',
    gpForm4: 'Mark file name:',
    gpForm5: 'Input file name',
    // sep3 Dialog
    step3dialog1: 'Dependency check',
    step3dialog2: 'Path',
    step3dialog3: 'Time',
    step3dialog4: 'Check Method',
    step3dialog5: 'Path',
    step3dialog6: 'An example path is as follows:',
    step3dialog7: 'Input the path where dependent data is located',
    step3dialog8: 'You can specify multiple continuous/discrete file paths as dependencies',
    step3dialog9: 'You can use the recommended values below to fill in',
    step3dialog10: 'Ready time',
    step3dialog11: 'Select a ready time',
    step3dialog12: 'Offset settings',
    step3dialog13: 'Scheduling period',
    step3dialog14: 'Select a scheduling period',
    step3dialog15: 'Single',
    step3dialog16:
      'It is for the "task scheduling period" of the dependent task. For example, if the scheduling period of the dependent task is "day", and the offset is set to "0", which means it depends on the instance of the dependent task on current day; the offset is set to " -1" means it depends on the instance of the dependent task on previous day',
    step3dialog17: 'Continuous',
    step3dialog18: 'It is for the "task scheduling period" of the dependent task. For example, if the scheduling period of the dependent task is "day", and the offset is set to "-3~-1", which means it depends on all instances of the dependent task in the past 3 days.',
    step3dialog19: 'Advanced',
    step3dialog20: 'It is for irregular dependencies. For example, if the current task scheduling period is inconsistent with the dependent task.',
    step3dialog21: 'Description',
    step3dialog22: 'Base offset',
    step3dialog23: 'No use',
    step3dialog24: 'Dependency',
    step3dialog25: '',
    step3dialog26: 'Select a ',
    step3dialog27: 'L stands for "Last". When L is set for "hour", it represents the last hour of the day, L-1 represents the 23rd hour of the day, and L+1 represents the first hour of 2nd day.',
    step3dialog28: 'Dependency preview',
    step3dialog29: 'Take the latest scheduling of the current task as an example',
    step3dialog30: 'Current time',
    step3dialog31: 'Task scheduled time',
    step3dialog32: 'Pre tasks',
    step3dialog33: 'Scheduled time',
    step3dialog34: 'It is natural month. 0 represents that it depends on instances of the dependent task in current month.',
    step3dialog35: 'It is natural week. In most scenarios there is no need to set this value. Only when you need to specify instance week of the dependent task, you can use it.',
    step3dialog36: 'It is Nnatural day. For example, when the above month and week are both 0, it depends on the instances of the Xth day of current month; if the month is -1 and the week is 0, it depends on the instaces of the Xth day of the previous month.',
    step3dialog37: `1) When task with large scheduling period depends on task with small period, for example, a monthly task depends on an hourly task, it is allowed to set specified values for the day and hour of the dependent task.
       2) When task with small scheduling period depends on task with large period, for example, hourly task depends on monthly task, there is no need to set a specified value for the month of the dependent task.`,
    // task editor
    taskEditor1: 'Code',
    taskEditor2: 'Verify',
    taskEditor3: 'Format',
    taskEditor4: 'Clear',
    taskEditor5: 'Git pull',
    taskEditor6: 'Tips',
    taskEditor7: 'Table',
    taskEditor8: '',
    taskEditor9: 'Verification result:',
    taskEditor10: 'Tomorrow, yesterday, etc. in the expression are all calculated based on ds which is time when parameter is used',
    taskEditor11: 'Parameter',
    taskEditor12: 'Meaning',
    taskEditor13: 'For other scenarios, please refer to:',
    taskEditor14: 'Datasource:',
    taskEditor15: 'Database:',
    taskEditor16: 'Click to generate a select SQL',
    taskEditor17: 'Column name',
    taskEditor18: 'Click to use this UDF',
    taskEditor19: 'Description',
    taskEditor20: 'Calling:',
    taskEditor21: 'Parameters:',
    taskEditor22: 'Returns:',
    // 具体模版step2
    // db->lakehouse
    highSetting: 'Advanced',
    region: 'Region',
    dbTip1: 'The region refers to the cloud region where the cluster which the task is submitted to for execution is located.',
    dbTip2: 'Select a region',
    dbToLakehouse2: 'Data source type',
    dbTip3: 'Select a data source',
    dbToLakehouse3: 'Table sync',
    dbContent1: 'Single table',
    dbContent2: 'Entire database or multiple tables',
    dbToLakehouse4: 'Sync type',
    dbContent3: 'Full data',
    dbContent4: 'Real time data',
    dbToLakehouse5: 'datasource',
    place_dbToLakehouse5: 'data source',
    dbContent5: 'Select a region first',
    dbContent6: 'Create a new data source',
    dbToLakehouse6: 'database',
    dbToLakehouse7: 'table',
    dbTip4: 'Select table name',
    dbContent7: 'Data preview',
    dbContent8: 'Source',
    dbContent9: 'Target',
    dbContent10: 'If the target table already exists, you should ensure that the schema of the target table covers the columns of the source table.',
    dbContent11: 'If the target table does not exist, you can input table name here. A new table with given name and schema will be created.',
    dbContent12: 'Select a database',
    dbContent13: 'Contact admin to create a new database',
    dbContent14: 'Create a new table',
    dbTip5: 'LakeHouse table name prefix (optional)',
    dbToLakehouse8: 'Partition column',
    dbContent15: 'If the partition column is given, data will be overwritten to the specified partition location every day; otherwise, data will be overwritten to the table location.',
    dbContent16: 'Partition name',
    dbContent17: 'Partition name',
    dbContent18: 'Tip',
    dbContent19: 'Source table',
    dbContent20: 'Description of situations of source table schema change',
    dbTip6: 'LakeHouse table name prefix (optional)',
    tempTable1: 'Column mapping',
    tempTable2: 'New',
    tempTable3: 'Refresh columns',
    dbTip7: 'Select a column',
    dbContent21: 'Mapping',
    dbContent22: 'Target table',
    tempTable4: 'Name mapping',
    tempTable5: 'Index',
    tempTable6: 'Name',
    tempTable7: 'Name',
    tempTable8: 'Type',
    tempTable9: 'Comment',
    tempTable10: 'Options',
    dbTable1: 'Preview',
    dbTable2: 'Mapping',
    dbCopy: 'Confirm to copy',
    dbSave: 'Save',
    dbContent23: 'Switch to this version',
    dbTip8: 'Full data synchronization',
    dbTip9: 'Pull all data from the source to LakeHouse periodically. DDL operation on the source table may affect the task execution.',
    dbTip10: 'Real time data synchronization',
    dbTip11: 'First pull all data from the source to LakeHouse, and then pull the incremental change to LakeHouse in real time. Real time sync is not supported for entire database or multi-tables. It requires to open the Binlog setting of MySQL client in advance.',
    dbTip12: 'Data preview',
    dbTip13: 'Only show 10 rows',
    dbValid1: 'Select a database',
    dbValid2: 'Select a region',
    dbValid3: 'Select a data source',
    dbValid4: 'Select a data source name',
    dbValid5: 'Select a database',
    dbValid6: 'Select a table',
    dbValid8: 'Input a theme',
    dbValid9: 'Input target table',
    dbValid10: 'Select the data source',
    dbValid11: 'Input a name',
    dbValid12: 'Select data sync method',
    dbValid13: '(Customize)',
    dbValid14: 'The name can only contain a-z, A-Z, 0-9 or - and should start with characters',
    dbTip14: 'Select at least 2 table mappings',
    dbTip15: 'The total must be 100%',
    dbTip16: 'Add at least one internal dependency',
    dbTip17: 'Select a column',
    dbTip18: 'Drag to sort manually',
    dbTable3: 'Function name',
    dbTable4: 'Parameter',
    dbTable5: 'Add mapping function',
    dbTable6: 'Succeeded',
    dbTable7: 'Failed',
    dbTip19: 'Column type',
    dbTip20: 'Select target column type',
    dbTip21: 'The decimal type supports user-defined precision and decimal places',
    dbTable8: 'Precision',
    dbTable9: 'Input the precision',
    dbTable10: 'Decimal places',
    dbTable11: 'Input the number of decimal places',
    dbTableInpt: 'Custom Table Name',
    dbTableSelect: 'Select existing table',
    // kafka Metis2Hive
    kfk1: 'Region',
    kfk2: 'Select a region',
    kfk3: 'Type',
    kfk4: 'Table',
    kfk5: 'Input topic',
    kfk6: 'Input the Kafka server address',
    kfk7: 'Kafka server',
    kfk8: 'Metadata display mode',
    kfk9: 'Column',
    kfk10: 'Input a column name',
    kfk11: 'Select column type',
    kfk12: 'Input column description',
    kfk13: 'Target table',
    kfk14: 'Region',
    kfk15: 'Select a region',
    kfk16: 'LakeHouse database',
    kfk17: 'Select a database',
    kfk18: 'LakeHouse table',
    kfk19: 'Select a table',
    kfk20: 'Next',
    kfk21: 'Confirm to copy',
    kfk22: 'Save',
    chooseRegion: 'Select a region',
    kfk23: 'Select how to display metadata',
    kfk24: 'Select a source region',
    kfk25: 'Select a group',
    kfk26: 'Input source table',
    kfk27: 'Select LakeHouse region',
    kfk28: 'Select LakeHouse database',
    kfk29: 'Select LakeHouse table',
    kfk30: 'Input topic',
    kfk31: 'Input the Kafka server address',
    kfk32: 'Visualization',
    kfk33: 'SQL',
    kfk34: 'Please add columns',
    kfk35: 'Please input SQL',
    kfk36: 'Back',
    // step2HCCloud DataMigration
    DM1: 'Sync data',
    DM2: 'Example：dt={{ yesterday_ds }}',
    DM3: 'Partition',
    DM4: 'Source path tips',
    DM5: 'Filter files in this directory',
    DM6: 'Filter file tips',
    DM7: 'Target table name is  generated based on source table',
    DM8: 'Target path tips',
    DM9: 'Before the task is executed, if you need to delete the file in the target directory with the same name as the source file, you can select "Yes", otherwise, select "No".',
    DM10: '1) Synchronization supports a single file and all files in a directory.',
    DM11: '2) When synchronizing a file, you need to fill in the path and the file name (including file extension), for example: s3://bucket name/data/shier/singleFile.csv. Note that path and file name are case-sensitive.',
    DM12: '3) When synchronizing a directory, the path needs to end with "/".',
    DM13: '4) You can add parameters in the path. Please refer to the "Parameter Tips".',
    DM14: '1) When the source path is a directory, if you need to filter the files in this directory, you can check this option and it supports regular expression matching. For example, when you only need to synchronize all files in a directory except files ending with "_Success".',
    DM15: 'Fill in “.*_Success” or “ab_Success” to match ‘ab_Success’. When synchronizing this directory, matched files will be excluded.',
    DM16: '1) You can add parameters in the path. Please refer to the "Parameter Tips".',
    DM17: '2) The path needs to end with "/". If not, it will recognize a directory ending with "/".',
    DM18: '3) It should be a single directory, such as',
    DM19: '',
    DM20: 'The region where the running cluster is located',
    DM21: 'Example: s3://bucket name/{{ yesterday_ds }}.strftime("%Y%m%d")/',
    DM22: 'Example: .*_SUCCESS',
    DM23: 'Example: s3://bucket name/xx/{{ yesterday_ds }}.strftime("%Y%m%d")/',
    DM24: 'Select source data region',
    DM25: 'Select target data region',
    DM26: 'Select source database',
    DM27: 'Select source table',
    DM28: 'Select task type',
    DM29: 'Input source path',
    DM30: 'Select whether to delete the target file',
    DM31: 'Input target path',
    DM32: 'Input filter expression',
    DM33: 'Huawei Singapore',
    DM34: 'If the source and target region are not in the same cloud, the default running cluster is located in Huawei Singapore; if in the same cloud, the default running cluster is in the region where the source data is located.',
    DM35: 'Grammar tips',
    // Hive2Mysql
    HM1: 'The name can only contain a-z, A-Z, 0-9, - or _ and should start with characters, up to 60 characters',
    HM2: 'Create a new table',
    HM3: 'Back',
    HM4: 'Data is overwritten to the mysql table or partition.',
    HM5: 'Input partition ID',
    HM6: 'Input table name',
    HM7: '--Pay attention to the time variable and as conversion. Refer to "Tips" 11 for details',
    HM8: '--Pay attention to the time variable and as conversion. Refer to "Tips" 22 for details',
    HM9: 'Please fill in required information!',
    // Hive2Clickhouse
    HC1: 'ClickHouse datasource',
    HC2: 'Database',
    HC3: 'Table',
    HC4: 'Partitions are obtained automatically',
    HC5: 'Refresh',
    HC6: 'Partition',
    HC7: 'Input the name',
    HC8: 'Input the type',
    HC9: '--Pay attention to time variables and as conversion. Refer to "Tips" for details',
    // lakehouse->doris  Hive2Doris
    HD1: 'Doris datasource',
    HD2: 'Database',
    HD3: 'Table',
    // lakehoue->redis
    LR1: 'Redis connection',
    LR2: 'Server',
    LR3: 'Port',
    LR4: 'Input the host, for example: 192.168.01',
    LR5: 'Input the port number, for example: 3306',
    LR6: 'Expiration time',
    LR7: 'Select the data source region',
    LR8: 'Input Redis server',
    LR9: 'Input the port number',
    LR10: 'Select expiration time',
    LR11: '--Pay attention to the time variable. Refer to "Tips" for details',
    LR12: 'At least one pre-dependency should be set',
    // Hive2Redshift
    HR1: 'Redshift connection',
    HR2: 'Server',
    HR3: 'Input the redshift address, for example: http://xxxx.ap_southeast-1.redshift.aws.com',
    HR4: 'Port',
    HR5: 'Input the port number, for example: 3306',
    HR6: 'Input aws iam_role information',
    HR7: 'Database',
    HR8: 'Input the database',
    HR9: 'Input Redshift server',
    HR10: 'Input the port number',
    HR11: 'Input aws iam_role information',
    HR12: 'Input the database',
    HR13: 'The logo path input format is incorrect.',
    HR14: 'Data dependency offset cannot be empty',
    // Hive2Sharestore
    HS1: 'Output',
    HS2: 'Type',
    HS3: 'S3/Obs path',
    HS4: 'For example: s3://shareit.tmp.ap-southeast-1/Default/BDP/dmp/dmp_to_sharestore',
    HS5: 'Database',
    HS6: 'View schema',
    HS7: 'Fill in the partition',
    HS8: 'Fill in the TiKV connection address',
    HS9: 'Fill in the target table name',
    HS10: 'Connection address',
    HS11: 'Table',
    HS12: 'Type',
    HS13: 'Please select type',
    HS14: 'Connection address',
    HS15: 'Cluster',
    HS16: 'Zookeeper',
    HS17: 'Table',
    HS18: 'Source field type',
    HS19: 'Input the correct S3/Obs file path starting with s3: or obs:',
    HS20: 'Target table name',
    HS21: 'Please select output data type',
    HS22: 'Please select data source type',
    HS23: 'Input the S3/Obs file path',
    HS24: 'Please select dataset region',
    HS25: 'Input the sharestore connection address',
    HS26: 'Input the sharestore cluster',
    HS27: 'Input the zookeeper address of sharestore',
    HS28: 'Input the target table name',
    HS29: 'Input TiKV connection address',
    HS30: 'Lakehouse table',
    HS31: 'S3/Obs file',
    // Hive2File
    HF1: 'Target',
    HF2: 'Type',
    HF3: 'Input email address. Separate multiple email addresses with comma',
    HF4: 'Input attachment format',
    HF5: 'Select a type',
    HF6: 'Input email theme',
    HF7: 'Input email text',
    HF8: 'Input attachment format',
    HF9: 'Input email address',
    HF10: 'Input the correct email format. Separate multiple email addresses with commas.',
    // flinkJar StreamingJAR
    FJ1: 'Upload jar',
    FJ2: 'Input jar address',
    FJ3: 'File',
    FJ4: 'Version',
    FJ5: 'Select jar file',
    FJ6: 'Select jar version',
    FJ7: 'Address',
    FJ8: 'Only the address uploaded in obs is supported, starting with HTTPS',
    FJ9: '--If you want to input code in multiple lines, you can end line with ',
    FJ10: 'Input an application name',
    FJ11: 'Select jar file',
    FJ12: 'Select jar version',
    FJ13: 'Input jar address',
    FJ14: 'Input the main class name',
    FJ15: 'Select cluster',
    FJ16: 'Input your email',
    FJ17: 'Input the topic name',
    FJ18: 'Input the KAFKA cluster',
    FJ19: 'Input the cooling time',
    FJ20: 'Input scaling tolerance',
    FJ21: 'Input the minimum degree of parallelism',
    FJ22: 'Input the maximum degree of parallelism',
    FJ23: 'Input consumer group',
    FJ24: 'Input delay time',
    FJ25: 'Input the scaling ratio',
    FJ26: 'Input the minimum number of delays',
    FJ27: 'Input the maximum number of delays',
    FJ28: 'Input the correct Main Class',
    // sparkjar SPARKJAR
    SJ1: 'Type',
    SJ2: 'Upload file',
    SJ3: 'Input file address',
    SJ4: 'File',
    SJ5: 'Version',
    SJ6: 'Address',
    SJ7: 'Support AWS s3 and Huawei cloud obs address',
    SJ8: '--hint--',
    SJ9: '--If you want to input code in multiple lines, you can end line with ',
    SJ10: 'Select file',
    SJ11: 'Select version',
    SJ12: 'Input address',
    // PythonShell
    PS1: 'Script type',
    PS2: 'Select script type',
    PS3: 'Image',
    PS4: 'Input the image address',
    PS5: 'Address',
    PS6: 'Image provides ',
    PS7: 'Libraries, resources, configuration and other software packages required when the task is run.',
    PS8: 'View tutorial',
    PS9: 'Input the image address',
    PS10: 'Image details',
    PS11: 'View image details',
    PS12: 'Dependent artifacts',
    PS13: 'The path of this artifact in the image is: /work/artifact_name. If the',
    PS14: ' script needs to rely on this artifact, please use the above path.',
    PS15: 'Dependent artifacts',
    PS16: 'Artifact version',
    PS17: '',
    PS18: 'Upload new artifacts',
    PS19: 'Input the execution command',
    PS20: 'Edit mode',
    PS21: 'Git pull',
    PS22: 'Root directory',
    PS23: 'Subpath',
    PS24: 'Inter the git subpath, for example: /a/b/test.py',
    PS25: 'Pull code',
    PS26: 'Select git I',
    PS27: 'Input git subpath',
    PS28: 'Select the version first!',
    // Flink SQL StreamingSQL
    FS1: 'Input dataset',
    FS2: 'Output dataset',
    FS3: 'Example parameters are as follows:',
    FS4: 'Select input dataset',
    FS5: 'Select output dataset',
    FS6: 'At least one dataset should be selected',
    HCtip: 'Please ensure that the database name already exists in the target area, otherwise execution will fail'
  }
};
